Namespace(lr=0.001, seed=19260817, data='books', epoch=20, trn_batch=256, tst_batch=256, con_batch=2048, test_frequency=1, max_seq_len=50, num_reco_neg=40, reg=1e-06, ssl_reg=0.01, latdim=32, mask_depth=3, path_prob=0.5, num_attention_heads=4, num_gcn_layers=2, num_trm_layers=2, load_model=None, num_mask_cand=50, mask_steps=10, eps=0.2, attention_probs_dropout_prob=0.3, hidden_dropout_prob=0.3, save_path='tem')
2023-07-06 06:04:55.029507: Start
2023-07-06 06:05:02.600956: Load Data
2023-07-06 06:05:02.601005: Users: 93043, Items(+1): 54756
2023-07-06 06:05:02.628702: Model Prepared
2023-07-06 06:05:02.628746: Model Initialized
Test result: h5=0.0896 n5=0.0542 h10=0.1461 n10=0.0725 h20=0.1634 n20=0.0771 h50=0.1670 n50=0.0778
2023-07-06 06:13:23.636345: Epoch 0/20, Train: loss = -64.9671, loss_main = 1.1203, loss_reco = -66.0502, loss_mask = -0.4037                   
Test result: h5=0.5274 n5=0.3984 h10=0.6579 n10=0.4406 h20=0.7851 n20=0.4729 h50=0.8451 n50=0.4855
2023-07-06 06:13:48.723379: Epoch 0/20, Test: hr@10 = 0.6579, ndcg@10 = 0.4406                   
2023-07-06 06:13:48.723433: Epoch 20/20, Best Result: hr@10 = 0.6579, ndcg@10 = 0.4406                   
2023-07-06 06:13:48.761243: Model Saved: tem

2023-07-06 06:21:37.101689: Epoch 1/20, Train: loss = -95.3804, loss_main = 0.8514, loss_reco = -96.1949, loss_mask = -0.4357                   
Test result: h5=0.6032 n5=0.4667 h10=0.7299 n10=0.5078 h20=0.8502 n20=0.5383 h50=0.9151 n50=0.5518
2023-07-06 06:22:01.461422: Epoch 1/20, Test: hr@10 = 0.7299, ndcg@10 = 0.5078                   
2023-07-06 06:22:01.461468: Epoch 20/20, Best Result: hr@10 = 0.7299, ndcg@10 = 0.5078                   
2023-07-06 06:22:01.497061: Model Saved: tem

2023-07-06 06:29:44.330119: Epoch 2/20, Train: loss = -107.5715, loss_main = 0.7204, loss_reco = -108.2585, loss_mask = -0.4165                   
Test result: h5=0.6258 n5=0.4930 h10=0.7425 n10=0.5308 h20=0.8391 n20=0.5554 h50=0.8890 n50=0.5658
2023-07-06 06:30:09.505114: Epoch 2/20, Test: hr@10 = 0.7425, ndcg@10 = 0.5308                   
2023-07-06 06:30:09.505164: Epoch 20/20, Best Result: hr@10 = 0.7425, ndcg@10 = 0.5308                   
2023-07-06 06:30:09.535567: Model Saved: tem

2023-07-06 06:37:40.803115: Epoch 3/20, Train: loss = -110.8199, loss_main = 0.6781, loss_reco = -111.4725, loss_mask = -0.3462                   
Test result: h5=0.6384 n5=0.5062 h10=0.7512 n10=0.5428 h20=0.8468 n20=0.5671 h50=0.8963 n50=0.5774
2023-07-06 06:38:03.905821: Epoch 3/20, Test: hr@10 = 0.7512, ndcg@10 = 0.5428                   
2023-07-06 06:38:03.905876: Epoch 20/20, Best Result: hr@10 = 0.7512, ndcg@10 = 0.5428                   
2023-07-06 06:38:03.939756: Model Saved: tem

2023-07-06 06:45:17.129719: Epoch 4/20, Train: loss = -112.8271, loss_main = 0.6566, loss_reco = -113.4426, loss_mask = -0.5114                   
Test result: h5=0.6432 n5=0.5117 h10=0.7541 n10=0.5476 h20=0.8589 n20=0.5743 h50=0.9200 n50=0.5869
2023-07-06 06:45:41.211561: Epoch 4/20, Test: hr@10 = 0.7541, ndcg@10 = 0.5476                   
2023-07-06 06:45:41.211632: Epoch 20/20, Best Result: hr@10 = 0.7541, ndcg@10 = 0.5476                   
2023-07-06 06:45:41.272769: Model Saved: tem

2023-07-06 06:53:12.359308: Epoch 5/20, Train: loss = -113.3482, loss_main = 0.6372, loss_reco = -113.9513, loss_mask = -0.4481                   
Test result: h5=0.6444 n5=0.5137 h10=0.7542 n10=0.5493 h20=0.8550 n20=0.5749 h50=0.9093 n50=0.5862
2023-07-06 06:53:38.943674: Epoch 5/20, Test: hr@10 = 0.7542, ndcg@10 = 0.5493                   
2023-07-06 06:53:38.943741: Epoch 20/20, Best Result: hr@10 = 0.7542, ndcg@10 = 0.5493                   
2023-07-06 06:53:38.975969: Model Saved: tem

2023-07-06 07:01:08.408157: Epoch 6/20, Train: loss = -114.0979, loss_main = 0.6267, loss_reco = -114.7028, loss_mask = -0.3298                   
Test result: h5=0.6478 n5=0.5177 h10=0.7572 n10=0.5531 h20=0.8551 n20=0.5780 h50=0.9085 n50=0.5891
2023-07-06 07:01:31.749560: Epoch 6/20, Test: hr@10 = 0.7572, ndcg@10 = 0.5531                   
2023-07-06 07:01:31.749612: Epoch 20/20, Best Result: hr@10 = 0.7572, ndcg@10 = 0.5531                   
2023-07-06 07:01:31.783107: Model Saved: tem

2023-07-06 07:08:52.861030: Epoch 7/20, Train: loss = -114.6964, loss_main = 0.6190, loss_reco = -115.2931, loss_mask = -0.3399                   
Test result: h5=0.6481 n5=0.5195 h10=0.7539 n10=0.5537 h20=0.8442 n20=0.5767 h50=0.8921 n50=0.5866
2023-07-06 07:09:17.847407: Epoch 7/20, Test: hr@10 = 0.7539, ndcg@10 = 0.5537                   

2023-07-06 07:16:36.330951: Epoch 8/20, Train: loss = -114.9401, loss_main = 0.6099, loss_reco = -115.5161, loss_mask = -0.4633                   
Test result: h5=0.6464 n5=0.5189 h10=0.7521 n10=0.5531 h20=0.8380 n20=0.5751 h50=0.8774 n50=0.5833
2023-07-06 07:16:59.693741: Epoch 8/20, Test: hr@10 = 0.7521, ndcg@10 = 0.5531                   

2023-07-06 07:24:24.976271: Epoch 9/20, Train: loss = -115.0627, loss_main = 0.6046, loss_reco = -115.6399, loss_mask = -0.4031                   
Test result: h5=0.6479 n5=0.5204 h10=0.7525 n10=0.5543 h20=0.8395 n20=0.5764 h50=0.8856 n50=0.5860
2023-07-06 07:24:49.136882: Epoch 9/20, Test: hr@10 = 0.7525, ndcg@10 = 0.5543                   

2023-07-06 07:32:07.935007: Epoch 10/20, Train: loss = inf, loss_main = 0.5985, loss_reco = -115.9415, loss_mask = inf                   
Test result: h5=0.6485 n5=0.5209 h10=0.7532 n10=0.5549 h20=0.8390 n20=0.5767 h50=0.8810 n50=0.5854
2023-07-06 07:32:32.533853: Epoch 10/20, Test: hr@10 = 0.7532, ndcg@10 = 0.5549                   

2023-07-06 07:40:01.851969: Epoch 11/20, Train: loss = -115.5651, loss_main = 0.5936, loss_reco = -116.1301, loss_mask = -0.4255                   
Test result: h5=0.6454 n5=0.5195 h10=0.7419 n10=0.5509 h20=0.8131 n20=0.5690 h50=0.8479 n50=0.5762
2023-07-06 07:40:28.307615: Epoch 11/20, Test: hr@10 = 0.7419, ndcg@10 = 0.5509                   

2023-07-06 07:47:58.216697: Epoch 12/20, Train: loss = -115.5013, loss_main = 0.5917, loss_reco = -116.0706, loss_mask = -0.3674                   
Test result: h5=0.6461 n5=0.5212 h10=0.7411 n10=0.5521 h20=0.8094 n20=0.5695 h50=0.8455 n50=0.5769
2023-07-06 07:48:21.099484: Epoch 12/20, Test: hr@10 = 0.7411, ndcg@10 = 0.5521                   

2023-07-06 07:56:19.277742: Epoch 13/20, Train: loss = -115.0626, loss_main = 0.5884, loss_reco = -115.6308, loss_mask = -0.3510                   
Test result: h5=0.6488 n5=0.5222 h10=0.7538 n10=0.5562 h20=0.8445 n20=0.5793 h50=0.8877 n50=0.5883
2023-07-06 07:56:43.522708: Epoch 13/20, Test: hr@10 = 0.7538, ndcg@10 = 0.5562                   

2023-07-06 08:04:01.284490: Epoch 14/20, Train: loss = inf, loss_main = 0.5844, loss_reco = -116.1563, loss_mask = inf                   
Test result: h5=0.6479 n5=0.5217 h10=0.7510 n10=0.5551 h20=0.8392 n20=0.5776 h50=0.8811 n50=0.5863
2023-07-06 08:04:24.173090: Epoch 14/20, Test: hr@10 = 0.7510, ndcg@10 = 0.5551                   

2023-07-06 08:11:50.045756: Epoch 15/20, Train: loss = -115.7551, loss_main = 0.5820, loss_reco = -116.3141, loss_mask = -0.3890                   
Test result: h5=0.6457 n5=0.5200 h10=0.7393 n10=0.5504 h20=0.8071 n20=0.5678 h50=0.8371 n50=0.5740
2023-07-06 08:12:13.335743: Epoch 15/20, Test: hr@10 = 0.7393, ndcg@10 = 0.5504                   

2023-07-06 08:19:27.361124: Epoch 16/20, Train: loss = -115.9231, loss_main = 0.5792, loss_reco = -116.4839, loss_mask = -0.3470                   
Test result: h5=0.6365 n5=0.5157 h10=0.7158 n10=0.5415 h20=0.7699 n20=0.5553 h50=0.7990 n50=0.5612
2023-07-06 08:19:50.480778: Epoch 16/20, Test: hr@10 = 0.7158, ndcg@10 = 0.5415                   

2023-07-06 08:27:22.560105: Epoch 17/20, Train: loss = -115.5501, loss_main = 0.5793, loss_reco = -116.1116, loss_mask = -0.3457                   
Test result: h5=0.6459 n5=0.5215 h10=0.7365 n10=0.5509 h20=0.8033 n20=0.5679 h50=0.8387 n50=0.5752
2023-07-06 08:27:45.961592: Epoch 17/20, Test: hr@10 = 0.7365, ndcg@10 = 0.5509                   

2023-07-06 08:34:59.989133: Epoch 18/20, Train: loss = -115.7969, loss_main = 0.5752, loss_reco = -116.3613, loss_mask = -0.2796                   
Test result: h5=0.6475 n5=0.5222 h10=0.7420 n10=0.5530 h20=0.8163 n20=0.5719 h50=0.8543 n50=0.5798
2023-07-06 08:35:22.503243: Epoch 18/20, Test: hr@10 = 0.7420, ndcg@10 = 0.5530                   

2023-07-06 08:43:04.146929: Epoch 19/20, Train: loss = -115.4579, loss_main = 0.5721, loss_reco = -116.0056, loss_mask = -0.4214                   
Test result: h5=0.6375 n5=0.5170 h10=0.7168 n10=0.5429 h20=0.7698 n20=0.5565 h50=0.7979 n50=0.5622
2023-07-06 08:43:27.813726: Epoch 19/20, Test: hr@10 = 0.7168, ndcg@10 = 0.5429                   

Test result: h5=0.6375 n5=0.5170 h10=0.7168 n10=0.5429 h20=0.7698 n20=0.5565 h50=0.7979 n50=0.5622
2023-07-06 08:43:52.512785: Epoch 20/20, Test: hr@10 = 0.7168, ndcg@10 = 0.5429                   
2023-07-06 08:43:52.512819: Epoch 20/20, Best Result: hr@10 = 0.7572, ndcg@10 = 0.5531

__________________________
Namespace(lr=0.001, seed=19260817, data='toys', epoch=20, trn_batch=256, tst_batch=256, con_batch=2048, test_frequency=1, max_seq_len=50, num_reco_neg=40, reg=1e-06, ssl_reg=0.01, latdim=32, mask_depth=3, path_prob=0.5, num_attention_heads=4, num_gcn_layers=2, num_trm_layers=2, load_model=None, num_mask_cand=50, mask_steps=10, eps=0.2, attention_probs_dropout_prob=0.3, hidden_dropout_prob=0.3, save_path='tem')
2023-07-06 08:51:33.744232: Start
2023-07-06 08:51:41.505177: Load Data
2023-07-06 08:51:41.505230: Users: 116429, Items(+1): 54784
2023-07-06 08:51:41.522383: Model Prepared
2023-07-06 08:51:41.522422: Model Initialized
Test result: h5=0.1006 n5=0.0598 h10=0.1988 n10=0.0912 h20=0.3964 n20=0.1405 h50=0.6251 n50=0.1883
2023-07-06 09:01:52.136891: Epoch 0/20, Train: loss = -63.5639, loss_main = 1.1781, loss_reco = -64.7071, loss_mask = -0.3838                   
Test result: h5=0.3778 n5=0.2690 h10=0.4582 n10=0.2954 h20=0.4923 n20=0.3042 h50=0.5205 n50=0.3098
2023-07-06 09:02:21.155542: Epoch 0/20, Test: hr@10 = 0.4582, ndcg@10 = 0.2954                   
2023-07-06 09:02:21.156842: Epoch 20/20, Best Result: hr@10 = 0.4582, ndcg@10 = 0.2954                   
2023-07-06 09:02:21.200036: Model Saved: tem

2023-07-06 09:11:22.736925: Epoch 1/20, Train: loss = -92.7027, loss_main = 0.9655, loss_reco = -93.6346, loss_mask = -0.4075                   
Test result: h5=0.4324 n5=0.3219 h10=0.5370 n10=0.3559 h20=0.6091 n20=0.3743 h50=0.6490 n50=0.3824
2023-07-06 09:11:52.063517: Epoch 1/20, Test: hr@10 = 0.5370, ndcg@10 = 0.3559                   
2023-07-06 09:11:52.063564: Epoch 20/20, Best Result: hr@10 = 0.5370, ndcg@10 = 0.3559                   
2023-07-06 09:11:52.108842: Model Saved: tem

2023-07-06 09:20:55.135614: Epoch 2/20, Train: loss = -102.1060, loss_main = 0.8652, loss_reco = -102.9372, loss_mask = -0.4293                   
Test result: h5=0.4353 n5=0.3327 h10=0.5221 n10=0.3610 h20=0.5841 n20=0.3768 h50=0.6209 n50=0.3842
2023-07-06 09:21:26.214838: Epoch 2/20, Test: hr@10 = 0.5221, ndcg@10 = 0.3610                   

2023-07-06 09:30:01.732886: Epoch 3/20, Train: loss = -105.2789, loss_main = 0.8240, loss_reco = -106.0747, loss_mask = -0.3821                   
Test result: h5=0.4094 n5=0.3225 h10=0.4742 n10=0.3436 h20=0.5244 n20=0.3564 h50=0.5644 n50=0.3642
2023-07-06 09:30:31.172727: Epoch 3/20, Test: hr@10 = 0.4742, ndcg@10 = 0.3436                   

2023-07-06 09:39:10.676534: Epoch 4/20, Train: loss = inf, loss_main = 0.8010, loss_reco = -107.4793, loss_mask = inf                   
Test result: h5=0.4109 n5=0.3244 h10=0.4758 n10=0.3455 h20=0.5282 n20=0.3588 h50=0.5727 n50=0.3675
2023-07-06 09:39:39.799471: Epoch 4/20, Test: hr@10 = 0.4758, ndcg@10 = 0.3455                   

2023-07-06 09:48:28.375348: Epoch 5/20, Train: loss = -107.7359, loss_main = 0.7823, loss_reco = -108.4993, loss_mask = -0.3068                   
Test result: h5=0.4252 n5=0.3329 h10=0.5000 n10=0.3571 h20=0.5628 n20=0.3731 h50=0.6100 n50=0.3825
2023-07-06 09:48:58.023515: Epoch 5/20, Test: hr@10 = 0.5000, ndcg@10 = 0.3571                   

2023-07-06 09:57:59.079736: Epoch 6/20, Train: loss = -108.2174, loss_main = 0.7693, loss_reco = -108.9641, loss_mask = -0.3509                   
Test result: h5=0.4242 n5=0.3324 h10=0.5010 n10=0.3573 h20=0.5649 n20=0.3736 h50=0.6093 n50=0.3824
2023-07-06 09:58:28.116994: Epoch 6/20, Test: hr@10 = 0.5010, ndcg@10 = 0.3573                   

2023-07-06 10:07:11.466037: Epoch 7/20, Train: loss = inf, loss_main = 0.7609, loss_reco = -109.4441, loss_mask = inf                   
Test result: h5=0.4322 n5=0.3367 h10=0.5105 n10=0.3622 h20=0.5734 n20=0.3782 h50=0.6130 n50=0.3861
2023-07-06 10:07:41.491662: Epoch 7/20, Test: hr@10 = 0.5105, ndcg@10 = 0.3622                   

2023-07-06 10:16:25.017321: Epoch 8/20, Train: loss = -108.9752, loss_main = 0.7529, loss_reco = -109.7068, loss_mask = -0.3526                   
Test result: h5=0.4129 n5=0.3277 h10=0.4780 n10=0.3488 h20=0.5294 n20=0.3619 h50=0.5719 n50=0.3702
2023-07-06 10:16:54.429455: Epoch 8/20, Test: hr@10 = 0.4780, ndcg@10 = 0.3488                   

2023-07-06 10:25:38.172949: Epoch 9/20, Train: loss = -109.2088, loss_main = 0.7456, loss_reco = -109.9390, loss_mask = -0.3010                   
Test result: h5=0.4291 n5=0.3362 h10=0.5047 n10=0.3607 h20=0.5676 n20=0.3767 h50=0.6108 n50=0.3854
2023-07-06 10:26:07.827954: Epoch 9/20, Test: hr@10 = 0.5047, ndcg@10 = 0.3607                   

2023-07-06 10:34:10.127801: Epoch 10/20, Train: loss = -109.5629, loss_main = 0.7384, loss_reco = -110.2822, loss_mask = -0.3465                   
Test result: h5=0.4259 n5=0.3345 h10=0.4977 n10=0.3578 h20=0.5549 n20=0.3724 h50=0.5942 n50=0.3802
2023-07-06 10:34:39.393467: Epoch 10/20, Test: hr@10 = 0.4977, ndcg@10 = 0.3578                   

2023-07-06 10:42:44.816509: Epoch 11/20, Train: loss = -109.5763, loss_main = 0.7338, loss_reco = -110.2995, loss_mask = -0.2673                   
Test result: h5=0.4116 n5=0.3271 h10=0.4730 n10=0.3470 h20=0.5195 n20=0.3589 h50=0.5586 n50=0.3665
2023-07-06 10:43:14.053463: Epoch 11/20, Test: hr@10 = 0.4730, ndcg@10 = 0.3470                   

2023-07-06 10:51:41.852362: Epoch 12/20, Train: loss = -109.5536, loss_main = 0.7335, loss_reco = -110.2676, loss_mask = -0.3662                   
Test result: h5=0.4187 n5=0.3305 h10=0.4829 n10=0.3513 h20=0.5272 n20=0.3627 h50=0.5607 n50=0.3691
2023-07-06 10:52:10.953243: Epoch 12/20, Test: hr@10 = 0.4829, ndcg@10 = 0.3513                   

2023-07-06 11:00:58.115671: Epoch 13/20, Train: loss = -109.4248, loss_main = 0.7274, loss_reco = -110.1336, loss_mask = -0.3645                   
Test result: h5=0.4074 n5=0.3237 h10=0.4688 n10=0.3436 h20=0.5154 n20=0.3555 h50=0.5564 n50=0.3634
2023-07-06 11:01:27.310954: Epoch 13/20, Test: hr@10 = 0.4688, ndcg@10 = 0.3436                   

2023-07-06 11:10:17.125103: Epoch 14/20, Train: loss = -109.4586, loss_main = 0.7259, loss_reco = -110.1698, loss_mask = -0.3341                   
Test result: h5=0.4067 n5=0.3236 h10=0.4728 n10=0.3450 h20=0.5269 n20=0.3588 h50=0.5760 n50=0.3684
2023-07-06 11:10:46.471028: Epoch 14/20, Test: hr@10 = 0.4728, ndcg@10 = 0.3450                   

2023-07-06 11:19:33.865767: Epoch 15/20, Train: loss = -109.4715, loss_main = 0.7226, loss_reco = -110.1839, loss_mask = -0.2956                   
Test result: h5=0.4163 n5=0.3288 h10=0.4844 n10=0.3509 h20=0.5365 n20=0.3642 h50=0.5796 n50=0.3726
2023-07-06 11:20:03.999969: Epoch 15/20, Test: hr@10 = 0.4844, ndcg@10 = 0.3509                   

2023-07-06 11:28:27.872453: Epoch 16/20, Train: loss = -109.6275, loss_main = 0.7199, loss_reco = -110.3274, loss_mask = -0.4021                   
Test result: h5=0.4159 n5=0.3281 h10=0.4836 n10=0.3502 h20=0.5352 n20=0.3633 h50=0.5785 n50=0.3718
2023-07-06 11:28:57.623706: Epoch 16/20, Test: hr@10 = 0.4836, ndcg@10 = 0.3502                   

2023-07-06 11:37:49.420337: Epoch 17/20, Train: loss = -109.3765, loss_main = 0.7189, loss_reco = -110.0824, loss_mask = -0.3387                   
Test result: h5=0.4302 n5=0.3366 h10=0.5010 n10=0.3596 h20=0.5567 n20=0.3738 h50=0.5963 n50=0.3816
2023-07-06 11:38:18.512696: Epoch 17/20, Test: hr@10 = 0.5010, ndcg@10 = 0.3596                   

2023-07-06 11:46:55.129458: Epoch 18/20, Train: loss = -109.4662, loss_main = 0.7169, loss_reco = -110.1701, loss_mask = -0.3450                   
Test result: h5=0.4331 n5=0.3379 h10=0.5061 n10=0.3617 h20=0.5628 n20=0.3762 h50=0.5997 n50=0.3835
2023-07-06 11:47:24.276286: Epoch 18/20, Test: hr@10 = 0.5061, ndcg@10 = 0.3617                   

2023-07-06 11:55:53.055126: Epoch 19/20, Train: loss = -109.4595, loss_main = 0.7181, loss_reco = -110.1653, loss_mask = -0.3453                   
Test result: h5=0.4413 n5=0.3423 h10=0.5198 n10=0.3678 h20=0.5835 n20=0.3840 h50=0.6230 n50=0.3920
2023-07-06 11:56:22.594035: Epoch 19/20, Test: hr@10 = 0.5198, ndcg@10 = 0.3678                   

Test result: h5=0.4413 n5=0.3423 h10=0.5198 n10=0.3678 h20=0.5835 n20=0.3840 h50=0.6230 n50=0.3920
2023-07-06 11:56:51.882459: Epoch 20/20, Test: hr@10 = 0.5198, ndcg@10 = 0.3678                   
2023-07-06 11:56:51.882497: Epoch 20/20, Best Result: hr@10 = 0.5370, ndcg@10 = 0.3559                                      